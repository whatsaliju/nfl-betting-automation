name: 3. Market Update (with Line-Flip Detection)

on:
  schedule:
    - cron: '0 0 * * 5'   # Friday 12:00 AM UTC (Thu 7:00 PM ET)
    - cron: '0 1 * * 0'   # Sunday 1:00 AM UTC (Sat 8:00 PM ET)  
    - cron: '30 16 * * 0'  # Sunday 5:00 PM UTC (Sun 1:00 PM ET)
  workflow_dispatch:
    inputs:
      week:
        description: 'NFL Week/Round (1-18, WC, DIV, CONF, SB)'
        required: true
        type: string
      analysis_type:
        description: 'Override analysis type (e.g., update, initial, final)'
        required: false
        default: 'update'
        type: string

jobs:
  update_market_data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: main
          
      - name: Pull latest changes (before starting)
        run: git pull origin main
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          pip install pandas selenium lxml webdriver-manager
          sudo apt-get update
          sudo apt-get install -y chromium-browser 
      
      - name: Get Week Number
        id: get_week
        run: |
          WEEK=""
          # Priority 1: workflow_dispatch input
          if [ -n "${{ github.event.inputs.week }}" ]; then
            WEEK="${{ github.event.inputs.week }}"
          fi
          
          # Priority 2: Auto-detect from existing files if no input provided
          if [ -z "$WEEK" ]; then
            # Check for playoff weeks in chronological order: WC -> DIV -> CONF -> SB
            if [ -f "data/weekSB/weekSB_referees.csv" ]; then
              WEEK="SB"
            elif [ -f "data/weekCONF/weekCONF_referees.csv" ]; then
              WEEK="SB"  # Next week after CONF
            elif [ -f "data/weekDIV/weekDIV_referees.csv" ]; then
              WEEK="CONF"  # Next week after DIV
            elif [ -f "data/weekWC/weekWC_referees.csv" ]; then
              WEEK="DIV"  # Next week after WC
            else
              # Fall back to numeric weeks (regular season)
              WEEK=$(ls data/week*/week*_referees.csv 2>/dev/null | grep -oP 'week\K[0-9]+' | sort -n | tail -1)
              # If we have week 18, next should be WC
              if [ "$WEEK" = "18" ]; then
                WEEK="WC"
              fi
            fi
          fi

          # Ensure WEEK is an integer
          WEEK="${WEEK%.*}"

          echo "week=$WEEK" >> $GITHUB_OUTPUT
          echo "üìÖ Updating market data for Week $WEEK"

      - name: Create config folder
        run: mkdir -p config data/week${{ steps.get_week.outputs.week }}
      
      - name: Create Action Network Cookies
        env:
          COOKIES: ${{ secrets.ACTION_NETWORK_COOKIES }}
        run: echo "$COOKIES" > config/action_network_cookies.json
      
      - name: Scrape Action Network (All Markets)
        run: |
          echo "üìä Scraping Action Network..."
          python3 scrapers/action_network_scraper_cookies.py ${{ steps.get_week.outputs.week }}
      
      - name: Scrape Action Network Injuries & Weather
        run: |
          echo "ü©π Scraping Action Network injuries & weather..."
          python3 scrapers/action_network_injuries_weather.py ${{ steps.get_week.outputs.week }}
          
      - name: Scrape RotoWire Lineups
        run: |
          echo "üèà Scraping RotoWire lineups & injuries for Week ${{ steps.get_week.outputs.week }}..."
          python3 scrapers/rotowire_scraper.py ${{ steps.get_week.outputs.week }}
          
      - name: Detect Line Flips & Update SDQL
        id: line_flips
        env:
          GIMMETHEDOG_EMAIL: ${{ secrets.GIMMETHEDOG_EMAIL }}
          GIMMETHEDOG_PASSWORD: ${{ secrets.GIMMETHEDOG_PASSWORD }}
        run: |
          python3 << 'PYTHON_EOF'
          import sys
          import os
          import pandas as pd
          import re
          import datetime
          
          # Add scraper path to system path for imports
          sys.path.append('scrapers')
          
          # Import SDQL runner
          try:
              from sdql_test import run_sdql_queries
              from selenium import webdriver
              from selenium.webdriver.chrome.service import Service
              from webdriver_manager.chrome import ChromeDriverManager
              sdql_available = True
          except ImportError as e:
              print(f"WARNING: Could not import SDQL dependencies: {e}. SDQL updates will be skipped.")
              sdql_available = False
          
          week = "${{ steps.get_week.outputs.week }}"
          
          # --- DETERMINE ANALYSIS TYPE BASED ON SCHEDULE (NOT LINE FLIPS) ---
          now_utc = datetime.datetime.now(datetime.timezone.utc)
          hour_utc = now_utc.hour
          day_utc = now_utc.weekday()  # 0=Monday, 6=Sunday
          
          # Check if this is a scheduled cron run or manual trigger
          github_event_name = "${{ github.event_name }}"
          is_manual = github_event_name == "workflow_dispatch"
          manual_override = "${{ github.event.inputs.analysis_type }}"
          
          if is_manual and manual_override:
              # Manual trigger with explicit type
              analysis_type_output = manual_override
              print(f"üìå Manual trigger with analysis_type: {analysis_type_output}")
          else:
              # Determine from schedule based on cron times
              # cron: '0 0 * * 5'   # Friday 12:00 AM UTC (Thu 7:00 PM ET)
              # cron: '0 1 * * 0'   # Sunday 1:00 AM UTC (Sat 8:00 PM ET)  
              # cron: '30 16 * * 0' # Sunday 5:00 PM UTC (Sun 1:00 PM ET)
              
              if day_utc == 4 and hour_utc == 0:  # Friday 12:00 AM UTC = Thu 7 PM ET
                  analysis_type_output = "initial"
                  print("üìÖ Detected: INITIAL analysis (Thursday 7 PM ET)")
              elif day_utc == 6 and hour_utc == 1:  # Sunday 1:00 AM UTC = Sat 8 PM ET
                  analysis_type_output = "update"
                  print("üìÖ Detected: UPDATE analysis (Saturday 8 PM ET)")
              elif day_utc == 6 and hour_utc == 16:  # Sunday 5:00 PM UTC = Sun 1 PM ET
                  analysis_type_output = "final"
                  print("üìÖ Detected: FINAL analysis (Sunday 1 PM ET)")
              else:
                  # Fallback for manual runs without explicit type
                  analysis_type_output = "update"
                  print(f"‚ö†Ô∏è Unknown schedule time (UTC: Day={day_utc}, Hour={hour_utc}), defaulting to 'update'")
          
          # Initialize line flip tracking (separate from analysis type)
          flips_detected = "false"
          flip_count = 0
          
          # Load baseline queries (from initial scrape or previous market update)
          baseline_queries_path = f'data/week{week}/week{week}_queries.csv'
          try:
              baseline = pd.read_csv(baseline_queries_path)
              print(f"‚úÖ Loaded baseline queries from {baseline_queries_path}")
          except FileNotFoundError:
              print(f"‚ö†Ô∏è No baseline queries found at {baseline_queries_path}, skipping line-flip detection.")
              # Still output the analysis type even if no baseline
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"flips_detected={flips_detected}\n")
                  f.write(f"flip_count={flip_count}\n")
                  f.write(f"analysis_type={analysis_type_output}\n")
              sys.exit(0)
          except Exception as e:
              print(f"‚ùå Error loading baseline queries: {e}")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"flips_detected={flips_detected}\n")
                  f.write(f"flip_count={flip_count}\n")
                  f.write(f"analysis_type={analysis_type_output}\n")
              sys.exit(0)
          
          # Load COMPLETE Action Network dataset (preserve all games from initial scrape)
          try:
              action_files_in_data = [f for f in os.listdir('data') if f.startswith('action_all_markets_')]
              if not action_files_in_data:
                  raise FileNotFoundError("No 'action_all_markets_' files found in 'data' directory.")
              
              # Use the OLDEST file (Wednesday's complete scrape) instead of newest
              earliest_action_file = sorted(action_files_in_data)[0]
              complete_spreads_path = os.path.join('data', earliest_action_file)
              current_spreads = pd.read_csv(complete_spreads_path)
              current_spreads = current_spreads[current_spreads['Market'] == 'Spread']
              
              # Try to get updated lines for games that haven't started yet
              latest_files = [f for f in action_files_in_data if f > earliest_action_file]
              if latest_files:
                  latest_action_file = sorted(latest_files, reverse=True)[0]
                  latest_spreads_path = os.path.join('data', latest_action_file)
                  try:
                      latest_spreads = pd.read_csv(latest_spreads_path)
                      latest_spreads = latest_spreads[latest_spreads['Market'] == 'Spread']
                      
                      # Update lines for games that appear in both datasets (active games)
                      for _, latest_row in latest_spreads.iterrows():
                          matchup_mask = current_spreads['Matchup'] == latest_row['Matchup']
                          if matchup_mask.any():
                              current_spreads.loc[matchup_mask, 'Line'] = latest_row['Line']
                              current_spreads.loc[matchup_mask, 'Team'] = latest_row['Team']
                      print(f"‚úÖ Updated lines from {latest_spreads_path} for active games")
                  except Exception as e:
                      print(f"‚ö†Ô∏è Could not update with latest lines: {e}, using complete dataset only")
              
              print(f"üìä Using complete dataset from {complete_spreads_path} ({len(current_spreads)} games)")
              print(f"Games included: {current_spreads['Matchup'].tolist()}")
              
          except FileNotFoundError as e:
              print(f"‚ö†Ô∏è {e}, skipping line-flip detection.")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"flips_detected={flips_detected}\n")
                  f.write(f"flip_count={flip_count}\n")
                  f.write(f"analysis_type={analysis_type_output}\n")
              sys.exit(0)
          except Exception as e:
              print(f"‚ùå Error loading spreads: {e}")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"flips_detected={flips_detected}\n")
                  f.write(f"flip_count={flip_count}\n")
                  f.write(f"analysis_type={analysis_type_output}\n")
              sys.exit(0)
          
          # Team name mapping
          TEAM_FULL_TO_ABBR = {
              'Arizona Cardinals': 'ARI', 'Atlanta Falcons': 'ATL', 'Baltimore Ravens': 'BAL', 'Buffalo Bills': 'BUF',
              'Carolina Panthers': 'CAR', 'Chicago Bears': 'CHI', 'Cincinnati Bengals': 'CIN', 'Cleveland Browns': 'CLE',
              'Dallas Cowboys': 'DAL', 'Denver Broncos': 'DEN', 'Detroit Lions': 'DET', 'Green Bay Packers': 'GB',
              'Houston Texans': 'HOU', 'Indianapolis Colts': 'IND', 'Jacksonville Jaguars': 'JAX', 'Kansas City Chiefs': 'KC',
              'Las Vegas Raiders': 'LV', 'Los Angeles Chargers': 'LAC', 'Los Angeles Rams': 'LAR', 'Miami Dolphins': 'MIA',
              'Minnesota Vikings': 'MIN', 'New England Patriots': 'NE', 'New Orleans Saints': 'NO', 'New York Giants': 'NYG',
              'New York Jets': 'NYJ', 'Philadelphia Eagles': 'PHI', 'Pittsburgh Steelers': 'PIT', 'San Francisco 49ers': 'SF',
              'Seattle Seahawks': 'SEA', 'Tampa Bay Buccaneers': 'TB', 'Tennessee Titans': 'TEN', 'Washington Commanders': 'WAS'
          }
          
          def parse_spread(line_str):
              line_str = str(line_str).strip().upper()
              if "PICK" in line_str:
                  return 0.0
              match = re.search(r'([+-]?\d+\.?\d*)', line_str)
              return float(match.group(1)) if match else 0.0
          
          # Detect line flips
          flips = []
          
          if 'away' not in baseline.columns or 'home' not in baseline.columns or 'Matchup' not in current_spreads.columns:
              print("‚ö†Ô∏è Baseline or current spreads missing expected columns. Cannot detect flips.")
          else:
              for _, row in current_spreads.iterrows():
                  try:
                      matchup_parts = row['Matchup'].split('@')
                      if len(matchup_parts) != 2:
                          print(f"‚ö†Ô∏è Skipping malformed matchup: {row['Matchup']}")
                          continue
                      
                      away_full = matchup_parts[0].strip()
                      home_full = matchup_parts[1].strip()
                      
                      away_code = TEAM_FULL_TO_ABBR.get(away_full, away_full)
                      home_code = TEAM_FULL_TO_ABBR.get(home_full, home_full)
                      
                      current_spread_val = parse_spread(row['Line'])
                      
                      if current_spread_val < 0:
                          current_fav_code = away_code if row['Team'] == away_full else home_code
                      elif current_spread_val > 0:
                          current_fav_code = home_code if row['Team'] == away_full else away_code
                      else:
                          current_fav_code = 'PK'
                      
                      baseline_row = baseline[
                          (baseline['away'] == away_code) & 
                          (baseline['home'] == home_code)
                      ]
                      
                      if baseline_row.empty:
                          continue
                      
                      baseline_fav_code = baseline_row.iloc[0]['favorite']
                      baseline_spread_val = float(baseline_row.iloc[0]['spread'])
                      
                      baseline_fav_team = ''
                      if baseline_fav_code == 'AF':
                          baseline_fav_team = away_code
                      elif baseline_fav_code == 'HF':
                          baseline_fav_team = home_code
                      elif baseline_fav_code == 'PK':
                          baseline_fav_team = 'PK'
          
                      # Line flip detection
                      if baseline_fav_team != current_fav_code:
                          if not (baseline_fav_team == 'PK' and current_fav_code == 'PK'):
                              flip_count += 1
                              print(f"üö® LINE FLIP #{flip_count}: {away_code} @ {home_code}")
                              print(f"  Baseline: {baseline_fav_team} ({baseline_spread_val:+.1f})")
                              print(f"  Current:  {current_fav_code} ({current_spread_val:+.1f})")
                              
                              referee = baseline_row.iloc[0]['referee']
                              game_type = baseline_row.iloc[0]['game_type']
                              
                              new_sdql_fav_code = 'PK'
                              if current_fav_code == away_code: 
                                  new_sdql_fav_code = 'AF'
                              elif current_fav_code == home_code: 
                                  new_sdql_fav_code = 'HF'
                              
                              new_query = f"'{referee}' in officials and {new_sdql_fav_code} and {game_type} and REG and season>=2018"
                              
                              flips.append({
                                  'matchup': f"{away_code} @ {home_code}",
                                  'away_code': away_code,
                                  'home_code': home_code,
                                  'old_favorite_team': baseline_fav_team,
                                  'new_favorite_team': current_fav_code,
                                  'old_spread': baseline_spread_val,
                                  'new_spread': current_spread_val,
                                  'query': new_query,
                                  'referee': referee,
                                  'game_type': game_type
                              })
                              
                  except Exception as e:
                      print(f"‚ùå Error processing {row.get('Matchup', 'Unknown')}: {e}")
                      continue
          
          # --- HANDLE LINE FLIPS (INDEPENDENT OF ANALYSIS TYPE) ---
          if flips and sdql_available:
              flips_detected = "true"
              print(f"\nüìä Running {len(flips)} updated SDQL queries...")
              
              new_queries_list = [flip['query'] for flip in flips]
              
              run_sdql_queries(
                  email=os.environ["GIMMETHEDOG_EMAIL"],
                  password=os.environ["GIMMETHEDOG_PASSWORD"],
                  queries=new_queries_list,
                  headless=True,
                  output_dir='data/historical'
              )
              
              # Update baseline queries CSV
              for flip in flips:
                  mask = (baseline['away'] == flip['away_code']) & (baseline['home'] == flip['home_code'])
                  
                  new_sdql_fav_code = 'PK'
                  if flip['new_favorite_team'] == flip['away_code']: 
                      new_sdql_fav_code = 'AF'
                  elif flip['new_favorite_team'] == flip['home_code']: 
                      new_sdql_fav_code = 'HF'
          
                  baseline.loc[mask, 'favorite'] = new_sdql_fav_code
                  baseline.loc[mask, 'spread'] = flip['new_spread']
                  baseline.loc[mask, 'query'] = flip['query']
              
              baseline.to_csv(baseline_queries_path, index=False)
              
              flip_df = pd.DataFrame(flips)
              flip_df_path = f'data/week{week}/week{week}_line_flips_{datetime.datetime.now().strftime("%Y%m%d_%H%M")}.csv'
              flip_df.to_csv(flip_df_path, index=False)
              
              print(f"‚úÖ Updated SDQL data for {flip_count} flipped games")
              print(f"üìÑ Saved flip report to {flip_df_path}")
          else:
              print("‚úÖ No line flips detected or SDQL updates skipped.")
              flips_detected = "false"
          
          # --- OUTPUT FOR GITHUB ACTIONS ---
          print(f"\nüìã FINAL OUTPUT:")
          print(f"  analysis_type = {analysis_type_output}")
          print(f"  flips_detected = {flips_detected}")
          print(f"  flip_count = {flip_count}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"flips_detected={flips_detected}\n")
              f.write(f"flip_count={flip_count}\n")
              f.write(f"analysis_type={analysis_type_output}\n")
          PYTHON_EOF
      
      - name: Commit Updated Data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          FILES_TO_ADD=(
            "data/action_all_markets_*.csv"
            "data/action_injuries_*.csv"
            "data/action_weather_*.csv"
            "data/rotowire_lineups_*.csv"
            "data/week${{ steps.get_week.outputs.week }}/week${{ steps.get_week.outputs.week }}_queries.csv"
            "data/week${{ steps.get_week.outputs.week }}/week${{ steps.get_week.outputs.week }}_line_flips_*.csv"
            "data/historical/sdql_results.csv"
          )

          for pattern in "${FILES_TO_ADD[@]}"; do
            git add $pattern 2>/dev/null || echo "No files matching $pattern found to stage."
          done
          
          if git diff --cached --quiet; then
            echo "No changes to commit for market & lineup update. Skipping commit/push."
          else
            git commit -m "üìä Week ${{ steps.get_week.outputs.week }} market & lineup update (${{ steps.line_flips.outputs.flip_count }} line flips) - ${{ steps.line_flips.outputs.analysis_type }}"
            
            git fetch origin
            git rebase origin/main || {
                echo "Rebase conflict detected. Resolving by taking our newly generated data."
                git checkout --ours data/action_*.csv 2>/dev/null || true
                git checkout --ours data/rotowire_*.csv 2>/dev/null || true
                git checkout --ours data/week${{ steps.get_week.outputs.week }}/*.csv 2>/dev/null || true
                git checkout --ours data/historical/sdql_results.csv 2>/dev/null || true
                
                git add data/ 2>/dev/null || true
                git rebase --continue
            }
            
            git push --force-with-lease
          fi
      
      - name: Trigger Pro Analysis
        uses: peter-evans/repository-dispatch@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          event-type: market-data-ready
          client-payload: '{"week": "${{ steps.get_week.outputs.week }}", "analysis_type": "${{ steps.line_flips.outputs.analysis_type }}", "line_flips": "${{ steps.line_flips.outputs.flip_count }}"}'
      
      - name: Summary
        run: |
          echo "‚úÖ Market data update complete"
          echo "üìä Line flips detected: ${{ steps.line_flips.outputs.flip_count }}"
          echo "üëâ Triggering Pro Analysis with type: ${{ steps.line_flips.outputs.analysis_type }}"
          echo "üìÅ Updated files:"
          ls -lh data/action_all_markets_*.csv \
                    data/action_injuries_*.csv \
                    data/action_weather_*.csv \
                    data/rotowire_lineups_*.csv \
                    data/week${{ steps.get_week.outputs.week }}/week${{ steps.get_week.outputs.week }}_queries.csv \
                    data/historical/sdql_results.csv 2>/dev/null | tail -10
